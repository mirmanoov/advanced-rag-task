[
  {
    "url": "https://python.langchain.com/api_reference/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/docs/introduction/",
    "title": "/docs/introduction/",
    "content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/tutorials/",
    "title": "/docs/tutorials/",
    "content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-4.1\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-4.1\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/how_to/",
    "title": "/docs/how_to/",
    "content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/api_reference/langchain/",
    "title": "LangChain reference",
    "content": "LangChain reference\nWelcome to the\nLangChain\npackage reference documentation!\nMost users will primarily interact with the main\nlangchain\npackage, which provides the complete set of implementations for building LLM applications. The packages below form the foundation of the LangChain ecosystem, each serving a specific purpose in the architecture:\nlangchain\nThe main entrypoint containing all implementations you need for building applications with LLMs.\nReference\nlangchain-core\nCore interfaces and abstractions used across the LangChain ecosystem.\nReference\nlangchain-text-splitters\nText splitting utilities for document processing.\nReference\nlangchain-mcp-adapters\nMake MCP tools available in LangChain and LangGraph applications.\nReference\nlangchain-tests\nStandard tests suite used to validate LangChain integration package implementations.\nReference\nlangchain-classic\nLegacy\nlangchain\nimplementations and components.\nReference\nIntegration Packages\nLooking for integrations with specific providers and services? Check out the\nintegrations reference\nfor packages that connect with popular LLM providers, vector stores, tools, and other services.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/langgraph/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/deepagents/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/integrations/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/langsmith/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/docs/introduction/#content-area",
    "title": "/docs/introduction/",
    "content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/introduction/#create-an-agent",
    "title": "/docs/introduction/",
    "content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/introduction/#core-benefits",
    "title": "/docs/introduction/",
    "content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/tutorials/#content-area",
    "title": "/docs/tutorials/",
    "content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-4.1\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-4.1\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/tutorials/#overview",
    "title": "/docs/tutorials/",
    "content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-4.1\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-4.1\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/tutorials/#concepts",
    "title": "/docs/tutorials/",
    "content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-4.1\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-4.1\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/tutorials/#preview",
    "title": "/docs/tutorials/",
    "content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-4.1\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-4.1\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/tutorials/#setup",
    "title": "/docs/tutorials/",
    "content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-4.1\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-4.1\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/tutorials/#installation",
    "title": "/docs/tutorials/",
    "content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-4.1\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-4.1\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/tutorials/#langsmith",
    "title": "/docs/tutorials/",
    "content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-4.1\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-4.1\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/tutorials/#components",
    "title": "/docs/tutorials/",
    "content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-4.1\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-4.1\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/tutorials/#1-indexing",
    "title": "/docs/tutorials/",
    "content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-4.1\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-4.1\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/tutorials/#loading-documents",
    "title": "/docs/tutorials/",
    "content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-4.1\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-4.1\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/how_to/#content-area",
    "title": "/docs/how_to/",
    "content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/how_to/#create-an-agent",
    "title": "/docs/how_to/",
    "content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/how_to/#core-benefits",
    "title": "/docs/how_to/",
    "content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/api_reference/langchain/langchain/",
    "title": "LangChain reference",
    "content": "LangChain reference\nWelcome to the\nLangChain\npackage reference documentation!\nMost users will primarily interact with the main\nlangchain\npackage, which provides the complete set of implementations for building LLM applications. The packages below form the foundation of the LangChain ecosystem, each serving a specific purpose in the architecture:\nlangchain\nThe main entrypoint containing all implementations you need for building applications with LLMs.\nReference\nlangchain-core\nCore interfaces and abstractions used across the LangChain ecosystem.\nReference\nlangchain-text-splitters\nText splitting utilities for document processing.\nReference\nlangchain-mcp-adapters\nMake MCP tools available in LangChain and LangGraph applications.\nReference\nlangchain-tests\nStandard tests suite used to validate LangChain integration package implementations.\nReference\nlangchain-classic\nLegacy\nlangchain\nimplementations and components.\nReference\nIntegration Packages\nLooking for integrations with specific providers and services? Check out the\nintegrations reference\nfor packages that connect with popular LLM providers, vector stores, tools, and other services.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/langchain/agents/",
    "title": "LangChain reference",
    "content": "LangChain reference\nWelcome to the\nLangChain\npackage reference documentation!\nMost users will primarily interact with the main\nlangchain\npackage, which provides the complete set of implementations for building LLM applications. The packages below form the foundation of the LangChain ecosystem, each serving a specific purpose in the architecture:\nlangchain\nThe main entrypoint containing all implementations you need for building applications with LLMs.\nReference\nlangchain-core\nCore interfaces and abstractions used across the LangChain ecosystem.\nReference\nlangchain-text-splitters\nText splitting utilities for document processing.\nReference\nlangchain-mcp-adapters\nMake MCP tools available in LangChain and LangGraph applications.\nReference\nlangchain-tests\nStandard tests suite used to validate LangChain integration package implementations.\nReference\nlangchain-classic\nLegacy\nlangchain\nimplementations and components.\nReference\nIntegration Packages\nLooking for integrations with specific providers and services? Check out the\nintegrations reference\nfor packages that connect with popular LLM providers, vector stores, tools, and other services.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/langchain/middleware/",
    "title": "LangChain reference",
    "content": "LangChain reference\nWelcome to the\nLangChain\npackage reference documentation!\nMost users will primarily interact with the main\nlangchain\npackage, which provides the complete set of implementations for building LLM applications. The packages below form the foundation of the LangChain ecosystem, each serving a specific purpose in the architecture:\nlangchain\nThe main entrypoint containing all implementations you need for building applications with LLMs.\nReference\nlangchain-core\nCore interfaces and abstractions used across the LangChain ecosystem.\nReference\nlangchain-text-splitters\nText splitting utilities for document processing.\nReference\nlangchain-mcp-adapters\nMake MCP tools available in LangChain and LangGraph applications.\nReference\nlangchain-tests\nStandard tests suite used to validate LangChain integration package implementations.\nReference\nlangchain-classic\nLegacy\nlangchain\nimplementations and components.\nReference\nIntegration Packages\nLooking for integrations with specific providers and services? Check out the\nintegrations reference\nfor packages that connect with popular LLM providers, vector stores, tools, and other services.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/langchain/models/",
    "title": "LangChain reference",
    "content": "LangChain reference\nWelcome to the\nLangChain\npackage reference documentation!\nMost users will primarily interact with the main\nlangchain\npackage, which provides the complete set of implementations for building LLM applications. The packages below form the foundation of the LangChain ecosystem, each serving a specific purpose in the architecture:\nlangchain\nThe main entrypoint containing all implementations you need for building applications with LLMs.\nReference\nlangchain-core\nCore interfaces and abstractions used across the LangChain ecosystem.\nReference\nlangchain-text-splitters\nText splitting utilities for document processing.\nReference\nlangchain-mcp-adapters\nMake MCP tools available in LangChain and LangGraph applications.\nReference\nlangchain-tests\nStandard tests suite used to validate LangChain integration package implementations.\nReference\nlangchain-classic\nLegacy\nlangchain\nimplementations and components.\nReference\nIntegration Packages\nLooking for integrations with specific providers and services? Check out the\nintegrations reference\nfor packages that connect with popular LLM providers, vector stores, tools, and other services.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/langchain/messages/",
    "title": "LangChain reference",
    "content": "LangChain reference\nWelcome to the\nLangChain\npackage reference documentation!\nMost users will primarily interact with the main\nlangchain\npackage, which provides the complete set of implementations for building LLM applications. The packages below form the foundation of the LangChain ecosystem, each serving a specific purpose in the architecture:\nlangchain\nThe main entrypoint containing all implementations you need for building applications with LLMs.\nReference\nlangchain-core\nCore interfaces and abstractions used across the LangChain ecosystem.\nReference\nlangchain-text-splitters\nText splitting utilities for document processing.\nReference\nlangchain-mcp-adapters\nMake MCP tools available in LangChain and LangGraph applications.\nReference\nlangchain-tests\nStandard tests suite used to validate LangChain integration package implementations.\nReference\nlangchain-classic\nLegacy\nlangchain\nimplementations and components.\nReference\nIntegration Packages\nLooking for integrations with specific providers and services? Check out the\nintegrations reference\nfor packages that connect with popular LLM providers, vector stores, tools, and other services.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/langchain/tools/",
    "title": "LangChain reference",
    "content": "LangChain reference\nWelcome to the\nLangChain\npackage reference documentation!\nMost users will primarily interact with the main\nlangchain\npackage, which provides the complete set of implementations for building LLM applications. The packages below form the foundation of the LangChain ecosystem, each serving a specific purpose in the architecture:\nlangchain\nThe main entrypoint containing all implementations you need for building applications with LLMs.\nReference\nlangchain-core\nCore interfaces and abstractions used across the LangChain ecosystem.\nReference\nlangchain-text-splitters\nText splitting utilities for document processing.\nReference\nlangchain-mcp-adapters\nMake MCP tools available in LangChain and LangGraph applications.\nReference\nlangchain-tests\nStandard tests suite used to validate LangChain integration package implementations.\nReference\nlangchain-classic\nLegacy\nlangchain\nimplementations and components.\nReference\nIntegration Packages\nLooking for integrations with specific providers and services? Check out the\nintegrations reference\nfor packages that connect with popular LLM providers, vector stores, tools, and other services.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/langgraph/langchain/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/langgraph/langgraph/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/langgraph/deepagents/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/langgraph/integrations/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/langgraph/langsmith/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/deepagents/langchain/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/deepagents/langgraph/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/deepagents/deepagents/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/deepagents/integrations/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/deepagents/langsmith/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/integrations/langchain/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/integrations/langgraph/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/integrations/deepagents/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/integrations/integrations/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/integrations/langsmith/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/langsmith/langchain/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/langsmith/langgraph/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/langsmith/deepagents/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/langsmith/integrations/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/api_reference/langsmith/langsmith/",
    "title": "Get started",
    "content": "Get started\nWelcome to the\nLangChain\nPython reference documentation!\nThese pages detail the core interfaces you will use when building applications with LangChain and LangGraph. Each section covers a different part of the ecosystem. Use the navigation header to view documentation for specific packages.\nReference docs\nThis site contains\nPython reference documentation\n. You can find\nconceptual guides, tutorials, and more\nin the\nmain LangChain documentation site\n.\nJavaScript/TypeScript reference documentation is available on the\nJS/TS reference site\n.\nContributing and cross-referencing\nFor information on how these docs are built, how to contribute, and how to automatically cross-reference in your project(s), please see the\nREADME\n.\nOld versions\nAPI reference documentation for LangChain v0.3.x can be found at\nreference.langchain.com/v0.3/python/\n.\nBack to top"
  },
  {
    "url": "https://python.langchain.com/docs/tutorials/#splitting-documents",
    "title": "/docs/tutorials/",
    "content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-4.1\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-4.1\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/tutorials/#storing-documents",
    "title": "/docs/tutorials/",
    "content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-4.1\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-4.1\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/tutorials/#2-retrieval-and-generation",
    "title": "/docs/tutorials/",
    "content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-4.1\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-4.1\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/tutorials/#rag-agents",
    "title": "/docs/tutorials/",
    "content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-4.1\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-4.1\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/tutorials/#rag-chains",
    "title": "/docs/tutorials/",
    "content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-4.1\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-4.1\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI"
  },
  {
    "url": "https://python.langchain.com/docs/tutorials/#next-steps",
    "title": "/docs/tutorials/",
    "content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-4.1\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-4.1\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI"
  }
]